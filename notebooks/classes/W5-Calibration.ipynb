{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96055f94",
   "metadata": {},
   "source": [
    "# Калибровка классификаторов\n",
    "\n",
    "Проблема — классификатор выдает какие-то числа от 0 до 1 (или просто вещественные числа).\n",
    "Эти числа, на самом деле, *уверенность* классификатора, а мы бы хотели иметь *вероятности*.\n",
    "\n",
    "Для простоты давайте ограничемся задачей бинарнокй классификации\n",
    "\n",
    "\n",
    "## Как определить вероятность?\n",
    "\n",
    "Пусть $a(X)$ — предсказание классификатора.\n",
    "Мы бы хотели, чтобы выполнялось равенство $P(y | a(X) = \\hat{p}) = \\hat{p}$.\n",
    "На практике, однако, часто оказывается, что есть ровно один объект, для котрого выполняется $a(X) = \\hat{p}$, и поэтому говорить о вероятностях не имеет смысла.\n",
    "\n",
    "### Разбиение на группы\n",
    "Можно разбить отрезок $[0; 1]$ на подотрезки и посчитать в каждом из них среднее значение классифиатора и сравнить его с долей объектов класса 1.\n",
    "\n",
    "Кривую, построенную в таких координатах, называют *диаграммой калибровки*.\n",
    "\n",
    "У идеально откалиброванного классификатора диаграма представляет собой прямую $y = x$.\n",
    "\n",
    "(диаграмы взяты из Учебника Яндекса) \\[1\\]\n",
    "\n",
    "![Пример диаграммы калибровки](https://yastatic.net/s3/ml-handbook/admin/12_2_32a327a143.png)\n",
    "\n",
    "\n",
    "#### Пример слишком уверенного классификатора\n",
    "![overconfident](https://yastatic.net/s3/ml-handbook/admin/12_3_033f331fed.png)\n",
    "\n",
    "\n",
    "#### Пример неуверенного классификатора\n",
    "![underconfident](https://yastatic.net/s3/ml-handbook/admin/12_4_d557d0984e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706666f8",
   "metadata": {},
   "source": [
    "## Оценка качества калибровки\n",
    "\n",
    "### Expected calibration error \n",
    "Разобьем отрезок $[0; 1]$ на подотрезки $B_1, B_2, \\ldots, B_k$ и вычислим\n",
    "$$\n",
    "    \\sum\\limits_{j=1}^k \\frac{\\#B_j}{N} |\\bar{y}(B_i) - \\bar{a}(B_i)|,\n",
    "$$ \n",
    "где $\\#B_j$ - количетсво элементов с предсказаниями в отрезке $B_j$, $\\bar{y}(B_i)$ — их доля, приндалжещаих первому классу, $\\bar{a}(B_i)$ — среднее предсказание классификатора на них.\n",
    "\n",
    "\n",
    "### Max calibration error\n",
    "В обозначениях из предыдущего пункта вычислим\n",
    "$$\n",
    "    \\max\\limits_{j=1, 2,\\ldots, k} |\\bar{y}(B_i) - \\bar{a}(B_i)|.\n",
    "$$ \n",
    "\n",
    "Так же можно взять среднеквадратичное.\n",
    "\n",
    "\n",
    "### Brier score\n",
    "Вычислим \n",
    "$$\n",
    "    \\sum\\limits_{j=1}^N (y_j - a(x_j))^2\n",
    "$$\n",
    "или\n",
    "$$\n",
    "    \\sum\\limits_{j=1}^N \\left(y_j\\log a(x_j) + (1 - y_j)(\\log(1 - a(x_j))\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe5fa1",
   "metadata": {},
   "source": [
    "## Алгоритмы калибровки\n",
    "\n",
    "Функцию, которая из исходных преобразований, получает откалиброванные, назовем *функцией деформации*.\n",
    "\n",
    "### Гистограмная калибровка\n",
    "Разобьем отрезок на подотрезки $B_i, i=1, 2,\\ldots, k$, и если предсказание классификатора попало в $i$-отрезок, то будем предсказывать значение $\\theta_i$.\n",
    "Для подбора параметров $\\theta_i$ решается задача оптимизации\n",
    "$$\n",
    "    \\sum\\limits_{j=1}^k \\sum\\limits_{i: a(i)\\in B_j} (\\theta_j - y_i))^2 \\to\\min\n",
    "$$.\n",
    "Замечания:\n",
    "- надо как-то выбирать бины и их количество;\n",
    "- можно брать сумму модулей, а не сумму квадратов\n",
    "- можно выбирать бины равной длины, или разной (и нормировать каждое слагаемое на длину), или равномощным образом (и нормировать на число элементов)\n",
    "- конечное число выходов у итогового классификатора\n",
    "\n",
    "### Изотоническая регрессия\n",
    "Решаем похожую задачу, но теперь \n",
    "1) настраиваем границы бинов $b_1, b_2, \\ldots b_{k-2}$.\n",
    "2) накладываем ограничение $\\theta_1 < \\theta_2 < \\ldots < \\theta_k$.\n",
    "\n",
    "Функций деформации $g$ по-прежнему кусочно-постоянная.\n",
    "\n",
    "Решается оптимизационная задача\n",
    "$$\n",
    "    \\sum\\limits_{i=1}^N (y_i - g(a(x_i))^2 \\to\\min\n",
    "$$\n",
    "\n",
    "Может быть решена за линейное время с помощью алгоритма Pool Adjacent Violators Algorithm\n",
    "\n",
    "### Калибровка Платта\n",
    "Метод предложен в [4], изначально разрабатывался для SVM, но может быть применен с к другим алгоритмам.\n",
    "\n",
    "У нас есть способ, как превратить произвольное вещественное число в число из отрезка $[0; 1]$ — логистическая функция.\n",
    "Положим\n",
    "    $$P(y = 1 | a(x) = p) = \\frac{1}{1 + e^{-A \\cdot p -b}}.$$\n",
    "    \n",
    "Параметры $A$ и $B$ можно подобрать, оптимизирую кросс-энтропию на отложенной выборке (или кросс-валидацией).\n",
    "   \n",
    "   \n",
    "   \n",
    "### Beta-калибровка\n",
    "Функция деформация ищется в виде\n",
    "$$\n",
    "    \\mu(s; a, b, c) = \\cfrac{1}{1 + \\cfrac{1}{e^c\\frac{s^a}{(1 - s)^b}}},\n",
    "$$\n",
    "где $a, b, c$ — настраиваемые параметры.\n",
    "\n",
    "Авторы \\[5\\] показывают, что их можно получить, решив задачу логистичесокй регресии на признаках \n",
    "$\\ln(q_i)$ и $-\\ln(1 - q_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb79d01",
   "metadata": {},
   "source": [
    "## Многоклассовый случай\n",
    "### Softmax с температурой\n",
    "$$\n",
    "    a(x) = Softmax(\\frac{b_1}{T}, \\frac{b_2}{T}, \\ldots, \\frac{b_l}{T})\n",
    "$$\n",
    "\n",
    "### Калибровка с помощью распределение Дирехле \\[6\\]\n",
    "Распределение Дирехле\n",
    "$$\n",
    "    f(x_1, x_2, \\ldots, x_K; \\alpha_1, \\alpha_2, \\ldots, \\alpha_K) = \n",
    "    \\frac{1}{B(\\alpha_1, \\alpha_2, \\ldots, \\alpha_K)} \\prod\\limits_{i=1}^K x_i^{\\alpha_i - 1},\n",
    "$$\n",
    "где $B(\\alpha_1, \\alpha_2, \\ldots, \\alpha_K) = \n",
    "\\cfrac{\\prod\\limits_{i=1}^K \\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum\\limits_{j=1}^k \\alpha_j\\right)}$\n",
    "\n",
    "\n",
    "[Визуализация](https://chart-studio.plotly.com/~david_avakian/14.embed)\n",
    "\n",
    "\n",
    "Функция деформации может быть представлена в виде \n",
    "$$\n",
    "    \\mu(\\mathbf{q}; \\mathbf{W}, \\mathbf{b}) = Softmax(\\mathbf{W}\\ln \\mathbf{q} + \\mathbf{b})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a700e0",
   "metadata": {},
   "source": [
    "\n",
    "## Список литературы\n",
    "1. [Статья в учебнике Яндекса](https://academy.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti)\n",
    "2. [Статья от Александра Дьяконова](https://alexanderdyakonov.wordpress.com/2020/03/27/%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-%D0%BA%D0%B0%D0%BB%D0%B8%D0%B1%D1%80%D0%BE%D0%B2%D0%BA%D0%B8-%D1%83%D0%B2%D0%B5%D1%80%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8/)\n",
    "3. [Tutorial на ECML KDD 2020](https://classifier-calibration.github.io/)\n",
    "4. Platt, J. (1999). Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. Advances in Large Margin Classifiers (pp. 61–74).\n",
    "5. Kull, M., Silva Filho, T. M., Flach, P., et al.  Beyond sigmoids: How to obtain well-calibrated probabilities frombinary classifiers with beta calibration. Electronic Journalof Statistics, 11(2):5052–5080, 2017 [Link](https://research-information.bris.ac.uk/ws/portalfiles/portal/154625753/Full_text_PDF_final_published_version_.pdf)\n",
    "6. Kull, M. et. al. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration. InProceedings of Advances in Neural Information Processing Systems, pp. 12295–12305, 2019. [Link](https://arxiv.org/abs/1910.12656)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
